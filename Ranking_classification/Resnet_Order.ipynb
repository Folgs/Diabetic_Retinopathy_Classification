{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"to3LdzRaIOVr"},"outputs":[],"source":["import torch\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import torchvision.models as models\n","\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.datasets as datasets\n","\n","import shutil\n","\n","import copy\n","import os\n","import numpy as np\n","\n","import itertools\n","import torch\n","from torchvision.utils import make_grid\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","import datetime\n","from time import time\n","\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Subset\n","\n","import wandb\n","\n","\n","torch.set_grad_enabled(True)\n","%matplotlib inline\n","\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import itertools\n","\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKM7Xijfw9Kt"},"outputs":[],"source":["# We use wandb to log our results\n","wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"pvcsQ5ZIw9Kv"},"source":["# 1. Data preprocesing\n","Read the execution parameters. Preprocess the image ONLY if it's the first run. Construct the dataset and run an example image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhEVI_d6w9Kw"},"outputs":[],"source":["first_run = False\n","num_run = 101\n","epochs = 50\n","learning_rate= 1e-6\n","N_images = 1584\n","batch_size = 18\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bOjDerjw9Kx"},"outputs":[],"source":["# If there is GPU, then use GPU.\n","import torch\n","if(torch.cuda.is_available()):\n","    print(\"SI\")\n","else:\n","    print(\"NO\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RmhpRzmw9Kx"},"outputs":[],"source":["def crop_image_from_gray(img,tol=7):\n","    if img.ndim ==2:\n","        mask = img>tol\n","        return img[np.ix_(mask.any(1),mask.any(0))]\n","    elif img.ndim==3:\n","        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","        mask = gray_img>tol\n","        \n","        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n","        if (check_shape == 0): # image is too dark so that we crop out everything,\n","            return img # return original image\n","        else:\n","            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n","            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n","            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n","            img = np.stack([img1,img2,img3],axis=-1)\n","        return img\n","\n","def load_ben_color(image, sigmaX=10):\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = crop_image_from_gray(image)\n","    image = cv2.resize(image, (300, 300))\n","    image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n","        \n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cjs7krfw9Ky"},"outputs":[],"source":["# Preprocessing of images. Transformations.\n","# Do it only once.\n","if (first_run):\n","    image_data = pd.read_csv('dataset_prova/dataset_prova_dades_imatges.csv', sep = ';', encoding=\"latin-1\")\n","    transform = lambda y: transforms.Compose([ transforms.CenterCrop(y), transforms.Resize([300,300])]) \n","\n","    for i in range(0,N_images):\n","        ID_image,label = image_data.loc[i,['ID_Imatge','Retinopatia']]\n","        path = f\"dataset_prova/classe_{label}/{ID_image}\"\n","        image = Image.open(path)\n","        W, H = image.size\n","        image = transform(H)(image)\n","        image = np.array(image)\n","        image = load_ben_color(image)\n","        image = Image.fromarray(image)\n","        image.save(f\"dataset_processat/classe_{label}/{ID_image}\")\n","        print(f\"dataset_processat/classe_{label}/{ID_image}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Wv7PxThIYmg"},"outputs":[],"source":["class Diabetic_Retinopathy_Dataset(Dataset): \n","  def __init__(self):\n","    self.image_data = pd.read_csv('dataset_prova/dataset_prova_dades_imatges.csv', sep = ';', encoding=\"latin-1\")\n","    self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0,1)]) \n","\n","  def __getitem__(self, i):\n","    ID_imatge, label = self.image_data.loc[i,['ID_Imatge','Retinopatia']]\n","    path = f\"dataset_processat/classe_{label}/{ID_imatge}\"\n","    image = Image.open(path)\n","    image = self.transform(image)\n","    return image, label-1\n","\n","  def __len__(self):\n","    return len(self.image_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pr5mkq8ww9K0"},"outputs":[],"source":["dataset__training, dataset_validation = torch.utils.data.random_split(Diabetic_Retinopathy_Dataset(), [round(N_images*0.8),round(N_images*0.2)]) \n","\n","dataloader_training = DataLoader(dataset__training, batch_size=batch_size, shuffle=True, pin_memory= True)\n","dataloader_validating = DataLoader(dataset_validation, batch_size=batch_size, shuffle=False, pin_memory= True)\n","x, labels = next(iter(dataloader_training))\n","print(x.shape)\n","print(labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdMMdHZzIaMo"},"outputs":[],"source":["image_data = pd.read_csv('dataset_prova/dataset_prova_dades_imatges.csv', sep = ';', encoding=\"latin-1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xaF4Zm3rIcl1"},"outputs":[],"source":["i = 964\n","Id, y = image_data.loc[i,['ID_Imatge','Retinopatia']]\n","path = f\"dataset_processat_full/classe_{y}/{Id}\"\n","print(\"Class:\",y)\n","Image.open(path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgxgG7myImi7"},"outputs":[],"source":["dataset__training, dataset_validation = torch.utils.data.random_split(Diabetic_Retinopathy_Dataset(), [round(N_images*0.8),round(N_images*0.2)]) \n","\n","dataloader_training = DataLoader(dataset__training, batch_size=batch_size, shuffle=True, pin_memory= True)\n","dataloader_validating = DataLoader(dataset_validation, batch_size=batch_size, shuffle=False, pin_memory= True)\n","x, labels = next(iter(dataloader_training))\n","print(x.shape)\n","print(labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"O_TqbPFiw9K2"},"source":["# 2. Construct the model\n","\n","Use a Resnet34 pretrained with imagenet data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yckENmq-w9K2"},"outputs":[],"source":["def prediction2label(pred: np.ndarray):\n","    # Convert ordinal predictions to class labels\n","    return (pred > 0.5).cumprod(axis=1).sum(axis=1) - 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmrjo_05w9K3"},"outputs":[],"source":["criterion_ord = nn.MSELoss(reduction='none')\n","def ordinal_regression(predictions, targets):\n","    # Ordinal regression with encoding as in https://arxiv.org/pdf/0704.1028.pdf\n","\n","    # Create out modified target with [batch_size, num_labels] shape\n","    modified_target = torch.zeros_like(predictions) # vector de 0\n","\n","    # Fill in ordinal target function, i.e. 0 -> [1,0,0,...]\n","    for i, target in enumerate(targets):\n","        modified_target[i,0:target+1] = 1\n","\n","    return criterion_ord(predictions,modified_target).sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FDqhMIYZJESE"},"outputs":[],"source":["# Take the ResNet34 model\n","model = models.resnet18(pretrained = True).to(device)\n","\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Sequential(\n","    nn.Linear(num_ftrs,5),\n","    nn.Sigmoid()\n",")\n","\n","model = model.to(device)\n","\n","optimizer = optim.Adam(model.parameters(), learning_rate)\n","criterion = ordinal_regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kiNmc3S4Q-RM"},"outputs":[],"source":["def adjust_learning_rate(optimizer, epoch, original_lr):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n","    lr = original_lr * (0.1 ** (epoch // 30))\n","    # For some models, different parameters are in different groups with different lr\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","def correct_predictions(predicted_batch, label_batch):\n","  pred = prediction2label(predicted_batch)\n","  acum = pred.eq(label_batch.view_as(pred)).sum().item()\n","  return acum\n","\n","def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n","    torch.save(state, filename)\n","    \n","    # save an extra copy if it is the best model yet\n","    if is_best:\n","        shutil.copyfile(filename, 'model_best.pth.tar')  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGSs50a3w9K4"},"outputs":[],"source":["classes = ('1: Normal','2: RD Lleu','3: RD Moderada','4: RD Severa','5: Proliferativa')\n","def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","    fig = plt.figure(figsize=(8,8))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hutuAXqtK2xf"},"outputs":[],"source":["def train(train_loader, model, criterion, optimizer, device):\n","\n","    # Switch to train mode\n","    model.train()\n","    loss_mean = []\n","    acc_mean = []\n","\n","    for i, (images, target) in enumerate(train_loader):\n","        # Reset gradients\n","        optimizer.zero_grad()\n","\n","        #move images to gpu\n","        images = images.to(device)\n","        target = target.to(device)\n","\n","        # Compute output\n","        output = model(images)\n","\n","        loss = criterion(output,target)\n","\n","        acc = 100 * (correct_predictions(output, target) / images.shape[0])\n","        \n","        loss.backward()\n","\n","        loss_mean.append(loss.item())\n","        acc_mean.append(acc)\n","\n","        optimizer.step()\n","\n","    return np.mean(acc_mean), np.mean(loss_mean)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7FIwkZzhhzo"},"outputs":[],"source":["def validate(val_loader, model, criterion, device):\n","\n","    # Switch to evaluate mode\n","    model.eval()\n","\n","    # We will save the values of the accuracies in this list to return the mean of the whole dataset at the end\n","    acc = 0\n","    loss = []\n","\n","    y_pred = []\n","    y_true = []\n","\n","    with torch.no_grad():  # We do not need to compute gradients\n","        for i, (images, target) in enumerate(val_loader):\n","\n","            images = images.to(device)\n","            target = target.to(device)\n","\n","            output = model(images)\n","\n","            # loss\n","            loss.append(criterion(output, target).item())\n","\n","            # measure accuracy\n","            acc += correct_predictions(output, target)\n","\n","            # confusion matrix\n","            output = prediction2label(output).data.cpu().numpy()\n","            y_pred.extend(output) # Save Prediction\n","            \n","            target = target.data.cpu().numpy()\n","            y_true.extend(target) # Save Truth\n","\n","    cf_matrix = confusion_matrix(y_true, y_pred)\n","\n","    eval_acc = 100. * acc / len(val_loader.dataset)\n","\n","    return eval_acc, np.mean(loss), cf_matrix"]},{"cell_type":"markdown","metadata":{"id":"iPMzUHVfw9K5"},"source":["# 3.Execució"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePZG8JJAQc9W"},"outputs":[],"source":["wandb.finish() # This is needed just in case there was a wandb run from a previous execution\n","wandb.init(project=\"mnist_colab\")\n","wandb.run.name = f'run_{num_run}'\n","best_acc = 0.\n","for epoch in range(epochs):\n","    adjust_learning_rate(optimizer, epoch, learning_rate)\n","\n","    acc_tr, loss_tr = train(dataloader_training, model, criterion, optimizer, device)\n","\n","    wandb.log({'Training/accuracy':acc_tr}, epoch)\n","    wandb.log({'Training/loss':loss_tr}, epoch)\n","\n","    acc, loss, cf_matrix = validate(dataloader_validating, model, criterion,device)\n","\n","    is_best = acc > best_acc\n","    best_acc1 = max(acc, best_acc)\n","\n","    wandb.log({'Validation/accuracy': acc},epoch)\n","    wandb.log({'Validation/loss':loss},epoch)\n","    wandb.log({'Conf_matrix':plot_confusion_matrix(cf_matrix, classes,normalize=True)}, epoch) \n","\n","    print(\"Iteracio\", epoch,\"completada. Validate loss\", loss, \"train loss\", loss_tr, \"Validate acc\", acc, \"train acc\", acc_tr)\n","\n","    save_checkpoint({\n","        'epoch': epoch + 1,\n","        'arch': \"resnet34\",\n","        'state_dict': model.state_dict(),\n","        'best_acc1': best_acc1,\n","        'optimizer' : optimizer.state_dict(),\n","    }, is_best)\n","\n","\n","wandb.finish()\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Resnet_Order.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}